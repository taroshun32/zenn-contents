---
title: "[ã‚³ã‚¹ãƒˆå‰Šæ¸›] RDSã®ãƒ­ã‚°ã‚’S3ã«ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã§è‡ªå‹•è»¢é€ã™ã‚‹"
emoji: "ğŸ¤–"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["ã‚³ã‚¹ãƒˆå‰Šæ¸›", "AWS", "RDS", "LOG", "S3"]
publication_name: "nextbeat"
published: false
---

## æ¦‚è¦

RDSã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ä¸€å®šæœŸé–“å¾Œã«å‰Šé™¤ã•ã‚Œã¾ã™ã€‚
https://docs.aws.amazon.com/ja_jp/AmazonRDS/latest/AuroraUserGuide/USER_LogAccess.MySQL.LogFileSize.html

ç›£æŸ»ã‚„ãƒˆãƒ©ãƒ–ãƒ«å¯¾å¿œã®ãŸã‚ã«é•·æœŸä¿å­˜ã™ã‚‹å ´åˆã€CloudWatch Logsã‚’çµŒç”±ã—ã¦S3ã«ä¿å­˜ã™ã‚‹æ–¹æ³•ã‚’æ¡ç”¨ã™ã‚‹ã‚±ãƒ¼ã‚¹ãŒä¸€èˆ¬çš„ã ã¨æ€ã„ã¾ã™ãŒã€ã“ã®æ–¹æ³•ã§ã¯ãƒ‡ãƒ¼ã‚¿è»¢é€ã§ä¸­ã€…ã®ã‚³ã‚¹ãƒˆãŒç™ºç”Ÿã—ã¾ã™ã€‚

ã“ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›ã‚’ç›®çš„ã¨ã—ã¦ã€RDSã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥S3ã¸è»¢é€ã™ã‚‹è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’Pythonã§ä½œæˆã—ã¦ã¿ãŸã®ã§ã€è‡ªåˆ†ã¸ã®å‚™å¿˜éŒ²ã¨ã—ã¦æ®‹ã—ã¦ãŠãã¾ã™ã€‚

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

ä»Šå›å®Ÿè£…ã—ãŸã‚·ã‚¹ãƒ†ãƒ ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ä»¥ä¸‹ã§ã™ã€‚

1. AWS EventBridgeã§å®šæœŸçš„ã«Lambdaé–¢æ•°ã‚’ãƒˆãƒªã‚¬ãƒ¼
2. Lambdaé–¢æ•°ãŒRDSã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®å„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
3. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åœ§ç¸®ã—ã€S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ§‹é€ ã§S3ãƒã‚±ãƒƒãƒˆã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚

```
s3://{ãƒã‚±ãƒƒãƒˆå}/
    â”œâ”€â”€ {ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å}/
    â”‚   â””â”€â”€ audit/
    â”‚       â””â”€â”€ {å¹´}/
    â”‚           â””â”€â”€ {æœˆ}/
    â”‚               â””â”€â”€ {æ—¥}/
    â”‚                   â””â”€â”€ {æ™‚}/
    â”‚                       â””â”€â”€ {ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å}/
    â”‚                           â””â”€â”€ {ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å}.gz
    â””â”€â”€ {åˆ¥ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å}/
        â””â”€â”€ ...
```

ä¾‹ãˆã°ã€`my-rds-cluster`ã¨ã„ã†ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åã§ã€`instance-1`ã¨ã„ã†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åã®å ´åˆã€å®Ÿéš›ã®S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```
my-rds-cluster/audit/2024/10/01/12/instance-1/audit.log.2024-10-01-12-00.0.gz
```

## ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰

https://github.com/taroshun32/aurora-mysql-log-archive
SAMã‚’ä½¿ç”¨ã—ã¦Lambdaé–¢æ•°ã§ä½œæˆã—ã¦ã¾ã™ã€‚ä»Šå›ã¯å®Ÿè£…ã‚³ãƒ¼ãƒ‰ã®æ¦‚è¦ã ã‘ç°¡å˜ã«è§£èª¬ã—ã¾ã™ã€‚

## å®Ÿè£…ã®è©³ç´°

å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã§ã™ã€‚
:::details ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```python
import boto3
import gzip
import urllib.request
import os
from io import BytesIO
from datetime import datetime, timedelta
from botocore.awsrequest import AWSRequest
import botocore.auth as auth

def main(event, context):
    # å®šæ•°
    S3_BUCKET = os.getenv('S3_BUCKET')
    CLUSTERS  = os.getenv('CLUSTERS').split(',')

    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    rds_client = boto3.client('rds')
    s3_client  = boto3.client('s3')

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±
    session     = boto3.session.Session()
    region      = session.region_name
    credentials = session.get_credentials()

    # SigV4ç½²å
    sigv4auth = auth.SigV4Auth(credentials, 'rds', region)

    # æœ€çµ‚æ›´æ–°æ™‚åˆ»ãŒ2æ™‚é–“15åˆ†å‰ã‹ã‚‰1æ™‚é–“å‰ã¾ã§ã®ãƒ­ã‚°ã‚’å‡¦ç†å¯¾è±¡ã¨ã™ã‚‹
    now           = datetime.now()
    two_hours_ago = int((now - timedelta(hours=2, minutes=15)).timestamp() * 1000)
    one_hour_ago  = int((now - timedelta(hours=1)).timestamp() * 1000)

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸€è¦§ã‚’å–å¾—ã—ã€ãƒ«ãƒ¼ãƒ—å‡¦ç†
    clusters = rds_client.describe_db_clusters(
        Filters=[{
            'Name':   'db-cluster-id',
            'Values': CLUSTERS
        }]
    )
    for cluster in clusters['DBClusters']:
        cluster_name = cluster['DBClusterIdentifier']

        print(f"Processing cluster: {cluster_name}")

        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä¸€è¦§ã‚’å–å¾—ã—ã€ãƒ«ãƒ¼ãƒ—å‡¦ç†
        instances = cluster['DBClusterMembers']
        for instance in instances:
            instance_name = instance['DBInstanceIdentifier']
            print(f"Processing instance: {instance_name}")

            # å¯¾è±¡ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
            download_log_file_names = []
            marker = None
            while True:
                params = {
                    'DBInstanceIdentifier': instance_name,
                    'FileLastWritten':      two_hours_ago,
                    'MaxRecords':           256
                }
                if marker:
                    params['Marker'] = marker

                logs = rds_client.describe_db_log_files(**params)

                download_log_file_names.extend([
                    log['LogFileName'] for log in logs['DescribeDBLogFiles']
                    if two_hours_ago <= log['LastWritten'] < one_hour_ago and log['LogFileName'].startswith('audit/')
                ])

                marker = logs.get('Marker')
                if not marker:
                    break

            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ«ãƒ¼ãƒ—å‡¦ç†
            for log_file_name in download_log_file_names:
                print(f"Processing log file: {log_file_name}")

                # S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ã‚’ç”Ÿæˆ
                timestamp  = datetime.strptime(log_file_name.split('.')[3], '%Y-%m-%d-%H-%M')
                object_key = f"{cluster_name}/audit/{timestamp.year}/{timestamp.month:02}/{timestamp.day:02}/{timestamp.hour:02}/{instance_name}/{log_file_name.split('/')[-1]}.gz"

                # S3ã§åŒä¸€ã®ãƒ•ã‚¡ã‚¤ãƒ«åãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                try:
                    s3_client.head_object(Bucket=S3_BUCKET, Key=object_key)
                    print(f"File already exists in S3: {object_key}. Skipped.")
                    continue  # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                except s3_client.exceptions.ClientError as e:
                    pass # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€å‡¦ç†ã‚’ç¶šè¡Œ

                # ç½²åä»˜ãURLã‚’ç”Ÿæˆ
                host   = f"rds.{region}.amazonaws.com"
                url    = f"https://{host}/v13/downloadCompleteLogFile/{instance_name}/{log_file_name}"
                awsreq = AWSRequest(method='GET', url=url)
                sigv4auth.add_auth(awsreq)

                req = urllib.request.Request(url, headers={
                    'Authorization':        awsreq.headers['Authorization'],
                    'Host':                 host,
                    'X-Amz-Date':           awsreq.context['timestamp'],
                    'X-Amz-Security-Token': credentials.token
                })

                # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                with urllib.request.urlopen(req) as response:
                    log_data = response.read()

                # ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã®å ´åˆã¯é™¤å¤–
                if len(log_data) == 0:
                    print(f"Log file {log_file_name} is empty. Skipping.")
                    continue

                # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åœ§ç¸®
                compressed_data = BytesIO()
                with gzip.GzipFile(fileobj=compressed_data, mode='wb') as f:
                    f.write(log_data)
                compressed_data.seek(0)

                # S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                s3_client.upload_fileobj(
                    Fileobj=compressed_data,
                    Bucket=S3_BUCKET,
                    Key=object_key
                )

    return {
        'statusCode': 200,
        'body': 'Audit log file processing completed.'
    }
```
:::

å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æƒ…å ±ã‚’å–å¾—ã—ã¦äºŒé‡ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹ã—ã€å…¨ã¦ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å¯¾è±¡ã«ä»¥ä¸‹ã®æµã‚Œã§å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚

1. audit-logãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§å–å¾—
2. S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ã®ç”Ÿæˆã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯
3. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
4. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®åœ§ç¸®ã¨S3ã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

é †ç•ªã«è§£èª¬ã—ã¾ã™ã€‚

### 1. audit-logãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§å–å¾—

RDS APIã‚’ä½¿ç”¨ã—ã¦ç›´è¿‘æ›´æ–°ã•ã‚ŒãŸaudit-logãƒ•ã‚¡ã‚¤ãƒ«åã®ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚
ã¾ãŸã€æŒ‡å®šã—ãŸæ™‚é–“ç¯„å›²å†…ã«æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚

```python
# å¯¾è±¡ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
download_log_file_names = []
marker = None
while True:
    params = {
        'DBInstanceIdentifier': instance_name,
        'FileLastWritten':      two_hours_ago,
        'MaxRecords':           256
    }
    if marker:
        params['Marker'] = marker

    logs = rds_client.describe_db_log_files(**params)

    download_log_file_names.extend([
        log['LogFileName'] for log in logs['DescribeDBLogFiles']
        if two_hours_ago <= log['LastWritten'] < one_hour_ago and log['LogFileName'].startswith('audit/')
    ])

    marker = logs.get('Marker')
    if not marker:
        break

...
```

https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rds/client/describe_db_log_files.html

### 2. S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ã®ç”Ÿæˆã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯

å„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦ã€S3ä¸Šã§ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
S3ä¸Šã«åŒåã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã—ã€å­˜åœ¨ã™ã‚‹å ´åˆã¯å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚

```python
# S3ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ã‚’ç”Ÿæˆ
timestamp  = datetime.strptime(log_file_name.split('.')[3], '%Y-%m-%d-%H-%M')
object_key = f"{cluster_name}/audit/{timestamp.year}/{timestamp.month:02}/{timestamp.day:02}/{timestamp.hour:02}/{instance_name}/{log_file_name.split('/')[-1]}.gz"

# S3ã§åŒä¸€ã®ãƒ•ã‚¡ã‚¤ãƒ«åãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
try:
    s3_client.head_object(Bucket=S3_BUCKET, Key=object_key)
    print(f"File already exists in S3: {object_key}. Skipped.")
    continue  # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
except s3_client.exceptions.ClientError as e:
    pass # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€å‡¦ç†ã‚’ç¶šè¡Œ

...
```

### 3. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

RDSã‹ã‚‰ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«ã€ç½²åä»˜ãURLã‚’ç”Ÿæˆã—ã¾ã™ã€‚
ç”Ÿæˆã—ãŸURLã‚’ä½¿ç”¨ã—ã¦ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```python
# ç½²åä»˜ãURLã‚’ç”Ÿæˆ
host   = f"rds.{region}.amazonaws.com"
url    = f"https://{host}/v13/downloadCompleteLogFile/{instance_name}/{log_file_name}"
awsreq = AWSRequest(method='GET', url=url)
sigv4auth.add_auth(awsreq)

req = urllib.request.Request(url, headers={
    'Authorization':        awsreq.headers['Authorization'],
    'Host':                 host,
    'X-Amz-Date':           awsreq.context['timestamp'],
    'X-Amz-Security-Token': credentials.token
})

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
with urllib.request.urlopen(req) as response:
    log_data = response.read()

# ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã®å ´åˆã¯é™¤å¤–
if len(log_data) == 0:
    print(f"Log file {log_file_name} is empty. Skipping.")
    continue
```

ä»Šå›ä½¿ç”¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨APIã®è©³ç´°ã«é–¢ã—ã¦ã¯ä»¥ä¸‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
https://docs.aws.amazon.com/ja_jp/AmazonRDS/latest/AuroraUserGuide/DownloadCompleteDBLogFile.html

### 4. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®åœ§ç¸®ã¨S3ã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’gzipå½¢å¼ã§åœ§ç¸®ã—ã€S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```python
# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åœ§ç¸®
compressed_data = BytesIO()
with gzip.GzipFile(fileobj=compressed_data, mode='wb') as f:
    f.write(log_data)
compressed_data.seek(0)

# S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
s3_client.upload_fileobj(
    Fileobj=compressed_data,
    Bucket=S3_BUCKET,
    Key=object_key
)
```

https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/upload_fileobj.html

## ã¾ã¨ã‚

ä»Šå›ã®å®Ÿè£…ã«ã‚ˆã‚Šã€RDSã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥S3ã«è»¢é€ã™ã‚‹ã“ã¨ãŒã§ãã€ãƒ‡ãƒ¼ã‚¿è»¢é€ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
ã¾ãŸã€Athenaã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§å¤§é‡ã®ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«åˆ†æã—ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»ã‚„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãŸã‚ã®æ´å¯Ÿã‚’å¾—ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚

AWSã«ã¯ä»–ã«ã‚‚ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ä½™åœ°ãŒã‚ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã¯å¤šãã‚ã‚‹ã¨æ€ã†ã®ã§ã€ã‚µãƒ¼ãƒ“ã‚¹ã‚’æœ€å¤§é™ã«æ´»ç”¨ã—ã¤ã¤ã€ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆã‚’é€²ã‚ã¦ã„ããŸã„ã¨æ€ã„ã¾ã™ã€‚
